# -*- coding: utf-8 -*-
"""CarPricePredictor.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1r7F5Gv2UPSzdZ3s0eTaJCqgWfzZdt0Lk
"""

import pandas as pd
from google.colab import data_table

data_table.enable_dataframe_formatter()

dataset = pd.read_csv("automobili.csv")

dataset

# Utilizziamo la funzione isna() per trovare i valori nulli, e la funzione sum() per contarli
nan_mask = dataset.isna()
nan_count = nan_mask.sum()

nan_count

"""Non ci sono valori null, ma il il 30% dei dati per Levy è uguale a "-", quindi non conosciuto, dato l'elevato numero ho deciso di eliminarla come colonna."""

if "Levy" in dataset.columns:
    # Elimina la colonna "Levy"
    dataset = dataset.drop("Levy", axis=1)

dataset

"""Elimino la colonna "Engine Volume" e "" visto che è un dato che e facilmente ottenibile in fase di utilizzo del modello."""

if "Engine volume" in dataset.columns:
    # Elimina la colonna "Levy"
    dataset = dataset.drop("Engine volume", axis=1)

dataset

"""Elimino la colonna ID visto che non ha nessun valore predittivo"""

if "ID" in dataset.columns:
    # Elimina la colonna "Levy"
    dataset = dataset.drop("ID", axis=1)

dataset

# Elimina il suffisso " km" dalla parte destra della colonna Mileage
dataset['Mileage'] = dataset['Mileage'].str.rstrip(' km')

dataset

"""Verifico di aver eliminato tutte le occorrenze di " km" in Mileage."""

# Specifica il suffisso da cercare (ad esempio, " km")
suffisso_da_cercare = " km"

# Conta il numero di occorrenze del suffisso nella colonna Mileage
numero_occasioni = (dataset['Mileage'].str.contains(suffisso_da_cercare)).sum()

# Stampa il risultato
print(f"Numero di occorrenze di '{suffisso_da_cercare}' nella colonna Mileage: {numero_occasioni}")

#converto il dato Milages da stringa a numero
dataset['Mileage'] = pd.to_numeric(dataset['Mileage'], errors='coerce')

"""L'attributo dataset "Manufacturer" ha diversi valori unici i quali non rappresentano un informazione utile per effettuare una predizione quindi creo una nuova categorica Other nelle quali le accorpo."""

# Calcola il conteggio delle occorrenze di ciascun valore nella colonna 'Manufacturer'
manufacturer_counts = dataset['Manufacturer'].value_counts()

# Identifica i valori che si verificano meno di 5 volte
values_to_replace = manufacturer_counts[manufacturer_counts <= 10].index

# Sostituisci i valori con 'Other'
dataset['Manufacturer'] = dataset['Manufacturer'].replace(values_to_replace, 'Other')

dataset['Manufacturer'].value_counts()

"""Fare prima "One Hot Encoding" e il "Data Scaling"?
Dipende dalla situazione bisogna provare entrambi.
Provo prima a fare "Data Scaling" e poi "One Hot Encoding" quando avro bisogno di utilizzare un algoritmo che si basa solo su valori numerici.
Utilizzo Z-Score Normalization visto che per algoritmi come Regressione lineare risulta generalmente mogliore, visto che porta piu velocemente a convergenza.
"""

from sklearn.preprocessing import StandardScaler

# Seleziona solo le colonne numeriche su cui applicare lo Z-score
colonnes_numeriche = dataset.select_dtypes(include=['int64', 'float64']).columns

# Inizializza lo StandardScaler
scaler = StandardScaler()

# Applica lo Z-score alle colonne numeriche
dataset[colonnes_numeriche] = scaler.fit_transform(dataset[colonnes_numeriche])

dataset

"""Calcolo la correlazione di Spearman per determinare la correlazione delle variabili numeriche"""

from scipy.stats import pearsonr, spearmanr

# Seleziona solo le colonne numeriche
colonnes_numeriche = ['Mileage', 'Prod. year', 'Cylinders', 'Airbags']

# Calcola il coefficiente di correlazione di Pearson solo per le coppie che contengono "Price"
for colonna in colonnes_numeriche:
    correlation_spearman, p_value_spearman = spearmanr(dataset['Price'], dataset[colonna])
    print(f"Coefficiente di correlazione di Spearman tra Price e {colonna}: {correlation_spearman}\nP-value: {p_value_spearman}\n")

"""Ecco l'interpretazione dei risultati ottenuti:

Price e Mileage:
Coefficiente di Correlazione di Spearman: -0.20453684714429626
P-value: 8.898376748162581e-181

Interpretazione: Il coefficiente di correlazione di Spearman tra "Price" e "Mileage" è -0.2045, indicando una correlazione inversa. Questo significa che, in generale, all'aumentare del prezzo, la quantità di chilometri percorsi tende a diminuire. Il p-value è molto vicino a zero, indicando che questa correlazione è statisticamente significativa.

Price e Prod. year:
Coefficiente di Correlazione di Spearman: 0.2931
P-value: 0.0

Interpretazione: Il coefficiente di correlazione di Spearman tra "Price" e "Prod. year" è 0.2931, indicando una correlazione positiva. Questo suggerisce che, in generale, all'aumentare dell'anno di produzione, il prezzo tende a aumentare. Il p-value è molto vicino a zero, indicando che questa correlazione è statisticamente significativa.

Price e Cylinders:
Coefficiente di Correlazione di Spearman: -0.0310
P-value: 1.6974825617411518e-05

Interpretazione: Il coefficiente di correlazione di Spearman tra "Price" e "Cylinders" è -0.0310, indicando una correlazione inversa, anche se molto debole. Il p-value è basso, indicando che questa correlazione è statisticamente significativa, ma la forza della correlazione è debole.

Price e Airbags:
Coefficiente di Correlazione di Spearman: -0.0574
P-value: 1.7126214075366264e-15

Interpretazione: Il coefficiente di correlazione di Spearman tra "Price" e "Airbags" è -0.0574, indicando una correlazione inversa, ma anch'essa debole. Il p-value è molto basso, indicando che questa correlazione è statisticamente significativa.

In sintesi, questi risultati suggeriscono che le variabili "Mileage", "Prod. year", "Cylinders" e "Airbags" sono correlate al prezzo dell'auto, con differenti gradi di forza di correlazione. La direzione (positiva o negativa) indica come le variabili tendono a variare insieme. Il p-value basso suggerisce che queste correlazioni non sono dovute al caso.

Come criterio per determinare la correlazione tra la variabile Price che è nimerica e le variabili categoriche ho deciso di analizzare la varianza applicando la technica ANOVA.
"""

from scipy.stats import f_oneway

# Trova le categorie uniche di 'Manufacturer'
manufacturer_categories = dataset['Manufacturer'].unique()

# Crea una lista di serie contenenti i prezzi per ogni categoria
price_by_category = [dataset[dataset['Manufacturer'] == category]['Price'] for category in manufacturer_categories]

# Esegui l'ANOVA utilizzando le serie dei prezzi
f_statistic, p_value = f_oneway(*price_by_category)

# Stampa dei risultati
print(f"F-statistic: {f_statistic}")
print(f"P-value: {p_value}")

"""Attributo possibilmente utile, potenza predittiva non nulla."""

# Trova le categorie uniche di 'Model'
manufacturer_categories = dataset['Model'].unique()

# Crea una lista di serie contenenti i prezzi per ogni categoria
price_by_category = [dataset[dataset['Model'] == category]['Price'] for category in manufacturer_categories]

# Esegui l'ANOVA utilizzando le serie dei prezzi
f_statistic, p_value = f_oneway(*price_by_category)

# Stampa dei risultati
print(f"F-statistic: {f_statistic}")
print(f"P-value: {p_value}")

"""Dato l'elevato numero di dati unici, potenza predittiva nulla(su 19000 record 17077 categorie diverse).

"""

# Trova le categorie uniche di 'Category'
manufacturer_categories = dataset['Category'].unique()

# Crea una lista di serie contenenti i prezzi per ogni categoria
price_by_category = [dataset[dataset['Category'] == category]['Price'] for category in manufacturer_categories]

# Esegui l'ANOVA utilizzando le serie dei prezzi
f_statistic, p_value = f_oneway(*price_by_category)

# Stampa dei risultati
print(f"F-statistic: {f_statistic}")
print(f"P-value: {p_value}")

"""P-value inferoiore a 0.05 cio significa che probabilmente la correlazione tra la categoria ed il prezzo e molto alta quindi questo parametro ha un alto valore predittivo."""

# Trova le categorie uniche di 'Leather interior'
manufacturer_categories = dataset['Leather interior'].unique()

# Crea una lista di serie contenenti i prezzi per ogni categoria
price_by_category = [dataset[dataset['Leather interior'] == category]['Price'] for category in manufacturer_categories]

# Esegui l'ANOVA utilizzando le serie dei prezzi
f_statistic, p_value = f_oneway(*price_by_category)

# Stampa dei risultati
print(f"F-statistic: {f_statistic}")
print(f"P-value: {p_value}")

"""Poco significativa si puo eliminare"""

# Trova le categorie uniche di 'Fuel type'
manufacturer_categories = dataset['Fuel type'].unique()

# Crea una lista di serie contenenti i prezzi per ogni categoria
price_by_category = [dataset[dataset['Fuel type'] == category]['Price'] for category in manufacturer_categories]

# Esegui l'ANOVA utilizzando le serie dei prezzi
f_statistic, p_value = f_oneway(*price_by_category)

# Stampa dei risultati
print(f"F-statistic: {f_statistic}")
print(f"P-value: {p_value}")

"""P-value inferoiore a 0.05 cio significa che probabilmente la correlazione tra la categoria ed il prezzo e molto alta quindi questo parametro ha un alto valore predittivo."""

# Trova le categorie uniche di 'Gear box type'
manufacturer_categories = dataset['Gear box type'].unique()

# Crea una lista di serie contenenti i prezzi per ogni categoria
price_by_category = [dataset[dataset['Gear box type'] == category]['Price'] for category in manufacturer_categories]

# Esegui l'ANOVA utilizzando le serie dei prezzi
f_statistic, p_value = f_oneway(*price_by_category)

# Stampa dei risultati
print(f"F-statistic: {f_statistic}")
print(f"P-value: {p_value}")

"""P-value inferoiore a 0.05 cio significa che probabilmente la correlazione tra la categoria ed il prezzo e molto alta quindi questo parametro ha un alto valore predittivo."""

# Trova le categorie uniche di 'Drive wheels'
manufacturer_categories = dataset['Drive wheels'].unique()

# Crea una lista di serie contenenti i prezzi per ogni categoria
price_by_category = [dataset[dataset['Drive wheels'] == category]['Price'] for category in manufacturer_categories]

# Esegui l'ANOVA utilizzando le serie dei prezzi
f_statistic, p_value = f_oneway(*price_by_category)

# Stampa dei risultati
print(f"F-statistic: {f_statistic}")
print(f"P-value: {p_value}")

"""Poco significativa si puo eliminare"""

# Trova le categorie uniche di 'Doors'
manufacturer_categories = dataset['Doors'].unique()

# Crea una lista di serie contenenti i prezzi per ogni categoria
price_by_category = [dataset[dataset['Doors'] == category]['Price'] for category in manufacturer_categories]

# Esegui l'ANOVA utilizzando le serie dei prezzi
f_statistic, p_value = f_oneway(*price_by_category)

# Stampa dei risultati
print(f"F-statistic: {f_statistic}")
print(f"P-value: {p_value}")

"""P-value inferoiore a 0.05 cio significa che probabilmente la correlazione tra la categoria ed il prezzo e molto alta quindi questo parametro ha un alto valore predittivo."""

# Trova le categorie uniche di 'Wheel'
manufacturer_categories = dataset['Wheel'].unique()

# Crea una lista di serie contenenti i prezzi per ogni categoria
price_by_category = [dataset[dataset['Wheel'] == category]['Price'] for category in manufacturer_categories]

# Esegui l'ANOVA utilizzando le serie dei prezzi
f_statistic, p_value = f_oneway(*price_by_category)

# Stampa dei risultati
print(f"F-statistic: {f_statistic}")
print(f"P-value: {p_value}")

"""P-value inferoiore a 0.05 cio significa che probabilmente la correlazione tra la categoria ed il prezzo e molto alta quindi questo parametro ha un alto valore predittivo."""

# Trova le categorie uniche di 'Color'
manufacturer_categories = dataset['Color'].unique()

# Crea una lista di serie contenenti i prezzi per ogni categoria
price_by_category = [dataset[dataset['Color'] == category]['Price'] for category in manufacturer_categories]

# Esegui l'ANOVA utilizzando le serie dei prezzi
f_statistic, p_value = f_oneway(*price_by_category)

# Stampa dei risultati
print(f"F-statistic: {f_statistic}")
print(f"P-value: {p_value}")

"""Poco significativa si puo eliminare

Quindi dall'analisi della varianza delle variabili categoriche si è determinato che gli attributi: "Drive wheels", "Leather interior" e "Color" non risultano apparentemente significativi per la predizione della variabile target. Quindi possono essere Eliminati. Per quanto riguarda la variabile "Model" visto che a livello di significato nel dominio del problema è altamente influente ma allo stesso tempo il test statistico ha determinato che risulta essere poco influente, per determinarne l'utilita reale verranno creati due modelli e si testeranno le differenze a livello delle metriche predittive.
"""

# Colonnes da eliminare
colonne_da_elimare = ["Drive wheels", "Leather interior", "Color"]

# Elimina le colonne specificate
dataset = dataset.drop(colonne_da_elimare, axis=1)

dataset

"""Prima di poter applicare un algoritmo di Decision Three o di Regressione lineare bisogna trasformare le variabili categoriche in variabili numeriche."""

# Seleziona colonne categoriche
colonne_categoriche = dataset.select_dtypes(include=['object']).columns

# Applica One-Hot Encoding a ciascuna colonna categorica
for colonna in colonne_categoriche:
    # Usa get_dummies per ottenere le colonne codificate One-Hot
    df_encoded = pd.get_dummies(dataset[colonna], prefix=colonna)

    # Aggiungi le nuove colonne al DataFrame originale
    dataset = pd.concat([dataset, df_encoded], axis=1)

    # Rimuovi la colonna categorica originale
    dataset = dataset.drop(colonna, axis=1)

print("\nDataFrame dopo One-Hot Encoding:")
print(dataset)

from sklearn.model_selection import cross_val_score, KFold
from sklearn.tree import DecisionTreeRegressor
import numpy as np

# Creiamo un esempio di array 'y' (variabile target)
y = dataset['Price'].values

# Creiamo un modello di albero decisionale
modello_albero = DecisionTreeRegressor()

# Specifica il numero di fold per la K-Fold Cross-Validation (in questo caso, K=10)
kfold = KFold(n_splits=10, shuffle=True, random_state=42)

# Lista di tutte le colonne tranne 'Price'
features = [colonna for colonna in dataset.columns if colonna != 'Price']

# Esegui la K-Fold Cross-Validation escludendo la colonna target durante il testing
risultati = cross_val_score(modello_albero, dataset[features], y, cv=kfold, scoring='r2', error_score='raise')

# Stampa i risultati
print("R-squared per ciascun fold:", risultati)

print("R-squared medio:", np.mean(risultati))

import numpy as np
from sklearn.model_selection import cross_validate
from sklearn.tree import DecisionTreeRegressor
from sklearn.metrics import make_scorer, r2_score, mean_absolute_error, mean_squared_error, accuracy_score, precision_score

# Seleziona le feature e la variabile target
features = [colonna for colonna in dataset.columns if colonna != 'Price']
X = dataset[features]
y = dataset['Price']

# Modello di albero decisionale per la regressione
modello_albero_regressione = DecisionTreeRegressor()

# Definisci le metriche
metriche = {
    'R-squared': make_scorer(r2_score),
    'MAE': make_scorer(mean_absolute_error),
    'MSE': make_scorer(mean_squared_error),
    'RMSE': make_scorer(lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred))),
}

# Esegui la K-Fold Cross-Validation con diverse metriche
risultati = cross_validate(modello_albero_regressione, X, y, cv=10, scoring=metriche)

# Stampa i risultati
for metrica, valori in risultati.items():
    print(f"{metrica} medio: {np.mean(valori)}")

"""R-squared medio: La media del coefficiente di determinazione R-squared è molto bassa (-255.46), indicando che il modello di albero decisionale per la regressione potrebbe avere difficoltà a spiegare la variazione nella variabile target.

MAE medio: L'errore assoluto medio (MAE) medio è relativamente basso (0.058), indicando che il modello, in media, si discosta di circa 0.058 dalle osservazioni reali. Tuttavia, la direzione dell'errore dipende dal contesto del problema.

MSE medio: La media dell'errore quadratico medio (MSE) è relativamente alta (3.966), indicando che il modello potrebbe avere una varianza elevata rispetto ai dati di training.

RMSE medio: La media dell'errore quadratico medio radice (RMSE) è relativamente alta (1.301), indicando che il modello ha una radice quadrata media dell'errore quadratico maggiore rispetto alle unità della variabile target.
"""

import numpy as np
from sklearn.model_selection import cross_validate
from sklearn.linear_model import LinearRegression
from sklearn.metrics import make_scorer, r2_score, mean_absolute_error, mean_squared_error

# Modello di regressione lineare
modello_lineare_regressione = LinearRegression()

# Definisci le metriche
metriche = {
    'R-squared': make_scorer(r2_score),
    'MAE': make_scorer(mean_absolute_error),
    'MSE': make_scorer(mean_squared_error),
    'RMSE': make_scorer(lambda y_true, y_pred: np.sqrt(mean_squared_error(y_true, y_pred))),
}

# Esegui la K-Fold Cross-Validation con diverse metriche
risultati = cross_validate(modello_albero_regressione, X, y, cv=10, scoring=metriche)

# Stampa i risultati
for metrica, valori in risultati.items():
    print(f"{metrica} medio: {np.mean(valori)}")

"""tutte le metriche sono mogliorate risulta generalmente mogliore."""